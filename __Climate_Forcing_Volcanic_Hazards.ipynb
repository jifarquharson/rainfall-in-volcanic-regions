{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volcanic hazards will be exacerbated by climate change–driven increase in heavy rainfall\n",
    "### Jamie I. Farquharson1, Falk Amelung1\n",
    "1 Rosenstiel School of Marine and Atmospheric Science, University of Miami, Miami, FL, USA\n",
    "\n",
    "Corresponding author: james.farquharson@rsmas.miami.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import packages\n",
    "'''\n",
    "import datetime as dt\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import math\n",
    "\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopandas.geoseries import GeoSeries\n",
    "from geopandas.geodataframe import GeoDataFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib import rcParams\n",
    "plt.rcParams[\"font.family\"] = 'sans-serif'\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Computer Modern Sans serif']\n",
    "plt.rcParams[\"font.family\"] = 'sans-serif'\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.serif'] = ['Palatino']\n",
    "params = {'text.latex.preamble' : [r'\\usepackage{amsmath}', r'\\usepackage{amssymb}']}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "import matplotlib as matplotlib\n",
    "from matplotlib.patches import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from shapely import wkt\n",
    "from geopandas.tools import sjoin\n",
    "import geopandas\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy import sparse\n",
    "\n",
    "from decimal import Decimal\n",
    "import os\n",
    "import netCDF4\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['text.usetex'] = True \n",
    "matplotlib.rcParams['text.latex.preamble'] = [r'\\usepackage[cm]{sfmath}']\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = 'cm'\n",
    "plt.rcParams[\"font.family\"] = 'sans-serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/jamiefarquharson/Desktop/RSMAS/Eruption_ntbk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function converts mm input to inches (for plotting figures the correct size).\n",
    "'''\n",
    "\n",
    "def mm2inch(*tupl):\n",
    "    if isinstance(tupl[0], tuple):\n",
    "        return tuple(k*0.0393701 for k in tupl[0])\n",
    "    else:\n",
    "        return tuple(k*0.0393701 for k in tupl)\n",
    "    \n",
    "'''\n",
    "Function realigns subplots so that the upper right corners coincide\n",
    "'''\n",
    "\n",
    "def reposition(axis1, axis2):\n",
    "    plt.draw()\n",
    "    p1 = axis1.get_position()\n",
    "    p2 = axis2.get_position()\n",
    "    axis2.set_position([p1.x1-p2.width, p1.y1-p2.height, p2.width, p2.height])\n",
    "\n",
    "'''\n",
    "Function defines factorial of value x\n",
    "'''\n",
    "def fact(x):\n",
    "    return Decimal(math.factorial(x))\n",
    "\n",
    "'''\n",
    "Function returns max and min of a list\n",
    "'''\n",
    "def minmax(x):\n",
    "    x = sorted(x)\n",
    "    return (x[0],x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Re-orders multi-part names (for example, \"Fournaise, Piton de la\" becomes \"Piton de la Fournaise\")\n",
    "'''\n",
    "def string_rearranger(string):\n",
    "    old_string = string\n",
    "    if ', ' in old_string:\n",
    "        new_string = old_string.rpartition(', ')[2] +' '+ old_string.rpartition(', ')[0]\n",
    "    else:\n",
    "        new_string = old_string\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in perceptually uniform colourmaps from http://www.fabiocrameri.ch/colourmaps.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASYAAAAxCAYAAAB3YARNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAB7UlEQVR4nO3aTZKbMBAGUGFj7MwyN8zlckbzI8jCDBIDnspuevHexqpWSwjj+oqFm2VZEkAkl58+AMBXggkIRzAB4QgmIBzBBIQjmIBwBBMQjmACwmn/t7H5/af8E7N95Vlzu5aGa8m4pq3qa2+qa2/WbT117fb9undnuNzKrTVr/Vr1tl2Zb6t13Vpv21K7V3s9qnXdet7H/VhLKaVfJ70fVe1sPqWUPtb9drXb+br72rOvXU57P/fo6vnqO9mtW8fdpdlqj2v5CXRp3sbXlFNKKbV53GrNXMZp7LfhMj5fn1OZn4dnGU/DoT5Xvbnu3a0bj7V8fo08vM6z5PFQ+1qf13qu76E+e1Wf87RbczhD1ZuH4eTeqvlp2sZT/6oPfflu8ljm+77sMY2vZ9EP5flMU3luQ/VYPsdTrmpl25Rzc6iP1fqpXCKNY+nN635jtW+uev8uz9L8hjcmIBzBBIQjmIBwBBMQjmACwhFMQDiCCQhHMAHhCCYgHMEEhCOYgHAEExCOYALCEUxAOIIJCEcwAeEIJiAcwQSEI5iAcAQTEI5gAsIRTEA4ggkIRzAB4QgmIBzBBIQjmIBwBBMQjmACwmmWZfnpMwDseGMCwhFMQDiCCQhHMAHhCCYgHMEEhCOYgHAEExCOYALC+QdYTJRdijFv8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 354.331x39.3701 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_data = np.loadtxt(\"climate_mods/ScientificColourMaps6/vik/vik.txt\")\n",
    "vik_map = LinearSegmentedColormap.from_list(\"vik\", cm_data)\n",
    "vik_map_r = LinearSegmentedColormap.from_list(\"vik\", cm_data[::-1])\n",
    "x = np.linspace(0, 100, 100)[None, :]\n",
    "fig = plt.figure(1, mm2inch(90,10), dpi=100)\n",
    "plt.imshow(x, aspect=\"auto\",cmap=vik_map)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASYAAAAxCAYAAAB3YARNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAB50lEQVR4nO3ay5HaQBAGYL2FU3CCDsMJOg3H4F30GB/EMr0WqvJt+/B9F7p6WqMRVP1woC2lNACZdF99AIB/CSYgHcEEpCOYgHQEE5COYALSEUxAOoIJSGf438Gf39vnPzH7qT1eb/1zvRtrxg1zrbvhqPvQ66f+tN40TTNM3akXZ/vpvG/3one+R/tir9dn+OhfnmEew3VHf5iH0Kv1ME+1P46PZ5zDXrdaD3W2n26PvcL1wy3U35512x/7dn1db7ox1POpLm09Y2nrPfbQ35ujv7X12ddmvKiP92op9folfOfdS93jbf+Y7UKvrdeF+s/WnntrfZy3UN+34/V9rX8YXsL6+7LX2eWYWbcwe6/rW+ivj/4art/D+naP/aPewmwJ59mWrc4+ZkrYa4/3WPdTP/bivp/7L2av7vHqDOv5eS7PEN+H5dzfLvb6/etH/UAv+MUEpCOYgHQEE5COYALSEUxAOoIJSEcwAekIJiAdwQSkI5iAdAQTkI5gAtIRTEA6gglIRzAB6QgmIB3BBKQjmIB0BBOQjmAC0hFMQDqCCUhHMAHpCCYgHcEEpCOYgHQEE5COYALSEUxAOm0p5avPAPCJX0xAOoIJSEcwAekIJiAdwQSkI5iAdAQTkI5gAtIRTEA6fwFK35pdqmwknAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 354.331x39.3701 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cm_data = np.loadtxt(\"climate_mods/ScientificColourMaps6/roma/roma.txt\")\n",
    "roma_map = LinearSegmentedColormap.from_list(\"roma\", cm_data)\n",
    "roma_map_r = LinearSegmentedColormap.from_list(\"roma\", cm_data[::-1])\n",
    "x = np.linspace(0, 10, 100)[None, :]\n",
    "fig = plt.figure(1, mm2inch(90,10), dpi=100)\n",
    "plt.imshow(x, aspect=\"auto\",cmap=roma_map)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Segment the \"vik\" colourmap'''\n",
    "\n",
    "cmaplist = [vik_map_r(i) for i in range(vik_map_r.N)]\n",
    "seg_cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, vik_map_r.N)\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(-18, 18, 13)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, vik_map_r.N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access volcano information and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-22 09:43:06--  https://webservices.volcano.si.edu/geoserver/GVP-VOTW/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=GVP_VOTW:Smithsonian_VOTW_Holocene_Eruptions&outputFormat=csv\n",
      "Resolving webservices.volcano.si.edu (webservices.volcano.si.edu)... 160.111.244.27\n",
      "Connecting to webservices.volcano.si.edu (webservices.volcano.si.edu)|160.111.244.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘Holocene_eruptions.csv’\n",
      "\n",
      "Holocene_eruptions.     [          <=>       ]   2.01M  1015KB/s    in 2.0s    \n",
      "\n",
      "2021-01-22 09:43:10 (1015 KB/s) - ‘Holocene_eruptions.csv’ saved [2112137]\n",
      "\n",
      "--2021-01-22 09:43:10--  https://webservices.volcano.si.edu/geoserver/GVP-VOTW/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=GVP_VOTW:Smithsonian_VOTW_Holocene_Volcanoes&outputFormat=csv\n",
      "Resolving webservices.volcano.si.edu (webservices.volcano.si.edu)... 160.111.244.27\n",
      "Connecting to webservices.volcano.si.edu (webservices.volcano.si.edu)|160.111.244.27|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘Holocene_volcanoes.csv’\n",
      "\n",
      "Holocene_volcanoes.     [       <=>          ]   2.05M  1.46MB/s    in 1.4s    \n",
      "\n",
      "2021-01-22 09:43:13 (1.46 MB/s) - ‘Holocene_volcanoes.csv’ saved [2146212]\n",
      "\n",
      "1419 volcano entries found, with a total of 11027 eruptions.\n",
      "923 distinct volcanoes identified.\n",
      "7 duplicate names:\n",
      "['Flores', 'Sumbing', 'Santa Isabel', 'Unnamed', 'Plosky', 'San Cristobal', 'Azul, Cerro']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Accesses the most recent GVP volcanoes eruptions lists, saves them as csv files: \"Holocene_volcanoes\" and \"Holocene_eruptions\".\n",
    "'''\n",
    "!wget --no-check-certificate --output-document Holocene_eruptions.csv \"https://webservices.volcano.si.edu/geoserver/GVP-VOTW/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=GVP_VOTW:Smithsonian_VOTW_Holocene_Eruptions&outputFormat=csv\"\n",
    "!wget --no-check-certificate --output-document Holocene_volcanoes.csv \"https://webservices.volcano.si.edu/geoserver/GVP-VOTW/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=GVP_VOTW:Smithsonian_VOTW_Holocene_Volcanoes&outputFormat=csv\"\n",
    "\n",
    "'''\n",
    "Read the downloaded csv file(s) into dataframe(s)\n",
    "'''\n",
    "volcano_list = pd.read_csv(\"Holocene_volcanoes.csv\")\n",
    "eruption_list = pd.read_csv(\"Holocene_eruptions.csv\")\n",
    "\n",
    "'''\n",
    "Update volcano names to avoid duplicates\n",
    "'''\n",
    "temp_list = list(volcano_list.Volcano_Name)\n",
    "duplicate_names = list(set([x for x in temp_list if temp_list.count(x) > 1]))\n",
    "duplicate_numbers = []\n",
    "for j in volcano_list.index:\n",
    "    if volcano_list.Volcano_Name[j] in duplicate_names:\n",
    "        volcano_list.Volcano_Name[j] = volcano_list.Volcano_Name[j]+' ' + str(volcano_list.Volcano_Number[j])\n",
    "        duplicate_numbers.append(volcano_list.Volcano_Number[j])\n",
    "len(set(duplicate_numbers))\n",
    "for j in eruption_list.index:\n",
    "    if eruption_list.Volcano_Number[j] in duplicate_numbers:\n",
    "        eruption_list.Volcano_Name[j] = eruption_list.Volcano_Name[j]+' ' + str(eruption_list.Volcano_Number[j])\n",
    "\n",
    "volcanoes = list(set(eruption_list.Volcano_Name)) ## {\n",
    "print('{} volcano entries found, with a total of {} eruptions.\\n{} distinct volcanoes identified.\\n{} duplicate names:\\n{}'.format(\n",
    "    len(volcano_list),len(eruption_list),len(volcanoes), len(duplicate_names), duplicate_names))                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "and\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Extracts latitude and longitude data for each volcano in 'volcano_list' [formatted in the dataframe as 'POINT (x.xx x.xxx)'] (WKT format?).\n",
    "Updates dataframe with extracted coordinates.\n",
    "'''\n",
    "\n",
    "lons = []\n",
    "lats = []\n",
    "for j,k in enumerate(volcano_list.GeoLocation):\n",
    "    lons.append(float(k[7:-1].split()[0])) # Extracts longitude as string, converts to float\n",
    "    lats.append(float(k[7:-1].split()[1])) # Extracts latitude as string, converts to float\n",
    "volcano_list['lats'] = lats # Adds latitudes to original dataframe\n",
    "volcano_list['lons'] = lons # Advods longitudes to original dataframe\n",
    "\n",
    "import fnmatch\n",
    "lst = list(set(volcano_list.Primary_Volcano_Type))\n",
    "filtered = fnmatch.filter(lst, 'Sub*')\n",
    "volcano_list = volcano_list.loc[~volcano_list[\"Primary_Volcano_Type\"].isin(filtered)]\n",
    "print('Done\\nand')\n",
    "\n",
    "'''\n",
    "Coordinates ready for plotting\n",
    "'''\n",
    "locations = volcano_list[['lats', 'lons']]\n",
    "locations[\"name\"] = volcano_list.Volcano_Name\n",
    "locations[\"number\"] = volcano_list.Volcano_Number\n",
    "locationlist = locations.values.tolist\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates dataframe with eruption information for all active volcanoes\n",
    "'''\n",
    "volcano_countries = volcano_list.set_index('Country')\n",
    "all_active_volcanoes = pd.merge(eruption_list,volcano_countries, how = 'inner', on = ['Volcano_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830 discrete volcanoes in unfiltered dataset\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dataframe contains Name, lat, and lon of each eruptive volcano\n",
    "'''\n",
    "\n",
    "all_volcano_set_name = []\n",
    "all_volcano_set_lat = []\n",
    "all_volcano_set_lon = []\n",
    "all_volcano_set_point = []\n",
    "\n",
    "for j, k in enumerate(all_active_volcanoes.Volcano_Name):\n",
    "    if k not in all_volcano_set_name:\n",
    "        all_volcano_set_name.append(k)\n",
    "        all_volcano_set_lat.append(all_active_volcanoes.lats[j])\n",
    "        all_volcano_set_lon.append(all_active_volcanoes.lons[j])\n",
    "        all_volcano_set_point.append(all_active_volcanoes.GeoLocation_y[j])\n",
    "        \n",
    "\n",
    "all_volcano_set = pd.DataFrame({'Name' :all_volcano_set_name,'lat' : all_volcano_set_lat, 'lon' : all_volcano_set_lon, 'point' : all_volcano_set_point})\n",
    "all_volcano_set.set_index('Name', inplace = True)\n",
    "print('{} discrete volcanoes in unfiltered dataset'.format(len(all_volcano_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataframe contains Name, lat, and lon of each eruptive volcano\n",
    "'''\n",
    "\n",
    "volcano_set_name = []\n",
    "volcano_set_lat = []\n",
    "volcano_set_lon = []\n",
    "\n",
    "for j, k in enumerate(all_active_volcanoes.Volcano_Name):\n",
    "    if k not in volcano_set_name:\n",
    "        volcano_set_name.append(k)\n",
    "        volcano_set_lat.append(all_active_volcanoes.lats[j])\n",
    "        volcano_set_lon.append(all_active_volcanoes.lons[j])\n",
    "\n",
    "volcano_set = pd.DataFrame({'Name' :volcano_set_name,'lat' : volcano_set_lat, 'lon' : volcano_set_lon})\n",
    "volcano_set.set_index('Name', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for reading and processing climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Models used '''\n",
    "fileNameCodes = [['NorESM1-M',\n",
    "  'CSIRO-Mk3-6-0',\n",
    "  'MRI-CGCM3',\n",
    "  'ACCESS1-3',\n",
    "  'inmcm4',\n",
    "  'MIROC5',\n",
    "  'IPSL-CM5A-MR',\n",
    "  'CanESM2',\n",
    "  'CNRM-CM5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_models(parameter = \"pr\", modelName =\"\"):\n",
    "    import glob as glob\n",
    "    models = [mod for mod in glob.glob(\"climate_mods/{}*\".format(parameter))]\n",
    "    modelList = []\n",
    "    for modelString in models:\n",
    "        if modelName in modelString:\n",
    "            modelList.append(modelString)\n",
    "    return sorted(modelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://stackoverflow.com/a/3431835/11191589''' ## Need to include hash list\n",
    "import hashlib\n",
    "def hash_bytestr_iter(bytesiter, hasher, ashexstr=False):\n",
    "    for block in bytesiter:\n",
    "        hasher.update(block)\n",
    "    return hasher.hexdigest() if ashexstr else hasher.digest()\n",
    "\n",
    "def file_as_blockiter(afile, blocksize=65536):\n",
    "    with afile:\n",
    "        block = afile.read(blocksize)\n",
    "        while len(block) > 0:\n",
    "            yield block\n",
    "            block = afile.read(blocksize)\n",
    "\n",
    "def checksum(parameter=\"pr\", modelName=\"\"):\n",
    "    fnamelst = list_of_models(parameter=parameter, modelName=modelName)\n",
    "    return [(fname, hash_bytestr_iter(file_as_blockiter(open(fname, 'rb')), hashlib.md5()))\n",
    "    for fname in fnamelst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x8b)\\xb5\\xaeh<\\x1dg\\xc9C\\x98\\x06\\xbf\\xde\\x9c\\xeb'\n",
      "b'\\x87\\xf2\\x16\\xabU\\xa1\\xab\\x88!BQt\\xcd\\xcc\\xd2\\xbd'\n",
      "b'.\\x99y\\x97\\xe2\\xa1\\x9c\\xf5\\xe1B\\xdf\\xbd\\x1b\\xbb\\xe3l'\n",
      "b'\\xe4\\xb3\\x15G\\xa3m\\x10\\x8a\\x08\\xecv\\xa9\\x80\\x8a\\xd5K'\n",
      "b'\\xbb\\xb0\\x19r\\x15\\x9dx/O\\xe7\\xc7\\xf2\\xff\\xd2\\xda\\xca'\n",
      "b'\\xdd[\\x8e\\x8d\\x93\\xf4-~3\\xca3)\\x88x\\xb6y'\n",
      "b\"\\xe3\\x1c\\xabI\\xf6=9I\\x89'4I\\xaa\\x1coe\"\n",
      "b'{\\x05\\x0b&\\xb2\\xd6\\x9d:\\x01\\xf3\\xd2\\x8eu\\xb6\"5'\n",
      "b'*\\xc0ZM\\xe4\\x83\\x18\\x9b\\xb1+\\x05\\x02\\xb4\\x01\\x0b\\x8a'\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    checkSum = checksum(parameter=\"pr\", modelName=fileNameCodes[0][i])\n",
    "    print(checkSum[0][1])\n",
    "# checkdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_info(parameter=\"pr\", modelName=\"\", checkSum=False):\n",
    "    import netCDF4\n",
    "    import hashlib\n",
    "    try:\n",
    "        modelFile = netCDF4.MFDataset(list_of_models(parameter=parameter, modelName=modelName))\n",
    "        if checkSum == False:\n",
    "            print('model: {}; var: {}; (time,lat,lon) : {}; (la,lo) : {:.1f} {:.1f}'.format(\n",
    "        modelFile.__dict__['model_id'],\n",
    "        modelFile.variables[parameter].long_name,\n",
    "        modelFile.variables[parameter].shape,\n",
    "        modelFile.variables['lat'][0],\n",
    "        modelFile.variables['lon'][0]\n",
    "    ))\n",
    "            modelFile.close()\n",
    "        else:\n",
    "            checksum = checksum(parameter=parameter, modelName=modelName)\n",
    "            print(checksum)\n",
    "\n",
    "# print(file_hash.digest())\n",
    "# print(file_hash.hexdigest())  # to get a printable str instead of bytes\n",
    "    except:\n",
    "        print(\"Error reading {}\".format(modelName)) ## checksum heree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lalo(parameter=\"pr\", modelName=\"\"):\n",
    "    import netCDF4\n",
    "    try:\n",
    "        modelFile = netCDF4.MFDataset(list_of_models(parameter=parameter, modelName=modelName))\n",
    "#         print(\"ok\")\n",
    "        return modelFile.variables[\"lat\"][:], modelFile.variables[\"lon\"][:]\n",
    "    except:\n",
    "        print(\"Error reading {}\".format(modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volcano_ij(volcano, lats, lons, verbose = False):\n",
    "    volcano = volcano\n",
    "    lon_vals = lons\n",
    "    lat_vals = lats\n",
    "    try:\n",
    "        name_volc = volcano\n",
    "        lat_volc = volcano_set.loc[[name_volc]].lat.values[0]\n",
    "        lon_volc = volcano_set.loc[[name_volc]].lon.values[0]+180\n",
    "\n",
    "        '''\n",
    "        Data are not infinite, so these lambda functions determine the latitude and longitute in the NCDF4 file(s) that\n",
    "        are *closest* to the precise coordinates of a given volcano. Volcano coordinates are defined above (lat_volc, lon_volc),\n",
    "        and typically correspond to the summit or centre of the caldera region.\n",
    "        '''\n",
    "        j_volc = min(lon_vals, key=lambda x:abs(x-lon_volc))\n",
    "        i_volc = min(lat_vals, key=lambda x:abs(x-lat_volc))\n",
    "        if verbose == True:\n",
    "            print(\"({:.1f},{:.1f}) --> ({:.1f},{:.1f})\".format(\n",
    "            lat_volc,lon_volc, i_volc, j_volc))\n",
    "        return i_volc, j_volc\n",
    "    except:\n",
    "        print(\"Problem with {}\".format(string_rearranger(volcano)))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45.698692321777344, 58.125)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Convert geolocation to model-specific (i,j) coordinates''' \n",
    "lat, lon = model_lalo(parameter=\"pr\", modelName=fileNameCodes[0][1])\n",
    "volcano_ij(volcano = \"St. Helens\", lats = lat, lons = lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_mean_temp(modelName, method = \"max\", verbose = False):\n",
    "    ''' \"how\" can be either \"max\" or \"mean\" '''\n",
    "    import netCDF4\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        modelFile = netCDF4.MFDataset(list_of_models(parameter=\"ta\", modelName=modelName))\n",
    "#         print(\"imported\")\n",
    "        startString = modelFile.variables[\"time\"].units.split()[2] # Days since XXX date\n",
    "        start = datetime.strptime(startString, \"%Y-%m-%d\")\n",
    "        time = [start+timedelta(int(x)) for x in modelFile.variables[\"time\"][:]]\n",
    "        model_gmT = []\n",
    "#         print(\"ok\")\n",
    "        for x in range(modelFile.variables[\"ta\"].shape[0]):\n",
    "            model_gmT.append(np.nanmean(modelFile.variables[\"ta\"][x][0]))\n",
    "        gmT_df = pd.DataFrame(\n",
    "    {'ix':time,'date':time, ##\n",
    "     'temp': model_gmT\n",
    "    })\n",
    "        gmT_df=gmT_df.set_index('ix')\n",
    "        gmT_df.index = pd.to_datetime(gmT_df.index)\n",
    "        gmT = gmT_df.resample(\"Y\", how=method)\n",
    "#         gmT_av = gmT_df.resample(\"Y\", how='mean')\n",
    "\n",
    "        modelFile.close()\n",
    "        if verbose == True:\n",
    "            print(\"{} succesfully processed\".format(modelName))\n",
    "        return gmT, gmT_df\n",
    "    except:\n",
    "        print(\"Error reading {}\".format(modelName))\n",
    "\n",
    "def heavy_rainfall(i_volc, j_volc, modelName=\"\", method = \"max\", verbose = False):\n",
    "    ''' \"how\" can be either \"max\" or \"mean\" '''\n",
    "    import netCDF4\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        modelFile = netCDF4.MFDataset(list_of_models(parameter=\"pr\", modelName=modelName))\n",
    "        startString = modelFile.variables[\"time\"].units.split()[2] # Days since XXX date\n",
    "        start = datetime.strptime(startString, \"%Y-%m-%d\")\n",
    "        time = [start+timedelta(int(x)) for x in modelFile.variables[\"time\"][:]]\n",
    "        rx_volc = []\n",
    "        lat_vals = modelFile.variables[\"lat\"][:]\n",
    "        lon_vals = modelFile.variables[\"lon\"][:]\n",
    "#         print(\"ok to here1\")\n",
    "        i = int(np.where(lat_vals==i_volc)[0])\n",
    "#         print(\"ok to here2\")\n",
    "        j = int(np.where(lon_vals==j_volc)[0])\n",
    "#         print(\"ok to here3\")\n",
    "        prcp = list(modelFile.variables[\"pr\"][:,i,j])\n",
    "        temp_df = pd.DataFrame(\n",
    "    {'ix':time,'date':time, ##\n",
    "     'rainfall': prcp\n",
    "    })\n",
    "        temp_df=temp_df.set_index('ix')\n",
    "        temp_df.index = pd.to_datetime(temp_df.index)\n",
    "        RX1 = temp_df.resample(\"Y\", how=method)\n",
    "#         RX1_av = temp_df.resample(\"Y\", how='mean')\n",
    "#         m, c, r, p, s = linregress(gmT,temp_df.rainfall)  ### call in gmT?\n",
    "#         rx_volc.append((m*2100+c)/(m*2006+c))\n",
    "\n",
    "        modelFile.close()\n",
    "        if verbose == True:\n",
    "            print(\"{} succesfully processed\".format(modelName))\n",
    "        return temp_df, RX1\n",
    "    except:\n",
    "        print(\"Error reading {}\".format(modelName))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "volcano = \"Rabaul\"\n",
    "for i in range(1):\n",
    "    try:\n",
    "        gmt, gmt_df = global_mean_temp(fileNameCodes[0][i], method = \"max\", verbose=False)\n",
    "        \n",
    "        lat, lon = model_lalo(parameter=\"pr\", modelName=fileNameCodes[0][i])\n",
    "        i_volc, j_volc = volcano_ij(volcano = volcano, lats = lat, lons = lon, verbose = False)\n",
    "        rx_df, rx = heavy_rainfall(i_volc, j_volc, modelName=fileNameCodes[0][i], method = \"max\", verbose = False)\n",
    "\n",
    "        rolling_T = gmt.temp.rolling(30, min_periods =15).mean()\n",
    "        rolling_rx = rx.rainfall.rolling(30, min_periods=15).mean()\n",
    "        rT = [x-rolling_T[-80] for x in rolling_T if not math.isnan(x)]\n",
    "        rR = [((x-rolling_rx[-80])/rolling_rx[-80])*100 for x in rolling_rx if not math.isnan(x)]\n",
    "        plt.plot(rT, \n",
    "         rR, # 65 = len(datset) - window\n",
    "lw = .8, label = fileNameCodes[0][i]\n",
    "    )\n",
    "#         rT = [x for x in rolling_T]\n",
    "#         rR = [x for x in rolling_rx]\n",
    "\n",
    "        coef = np.polyfit(rT,rR,1)\n",
    "        poly1d_fn = np.poly1d(coef) \n",
    "        # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "\n",
    "        plt.plot(rT, poly1d_fn(rT), '--k')\n",
    "    except:\n",
    "        print(\"issue with {}\".format(fileNameCodes[0][i]))\n",
    "        continue\n",
    "plt.legend()\n",
    "plt.axhline(0, lw=.5, color = \"k\")\n",
    "plt.title(string_rearranger(volcano))\n",
    "plt.xlim(-1.5,5.5)\n",
    "plt.ylim(-17,32)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
